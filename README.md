# ğŸš€ Data Ingester ADB - Pipeline de Ingesta de Datos

[![Python](https://img.shields.io/badge/Python-3.12-blue.svg)](https://python.org)
[![Azure](https://img.shields.io/badge/Azure-Data%20Factory-0078D4)](https://azure.microsoft.com/services/data-factory/)
[![Databricks](https://img.shields.io/badge/Databricks-Notebooks-FF3621)](https://databricks.com/)
[![GitHub Actions](https://img.shields.io/badge/GitHub%20Actions-CI%2FCD-2088FF)](https://github.com/features/actions)

Sistema completo de ingesta y transformaciÃ³n de datos que automatiza la extracciÃ³n de datasets desde Kaggle hacia Azure Data Lake Storage (ADLS) y su posterior procesamiento a travÃ©s de una arquitectura medallion (Bronze-Silver-Gold) usando Azure Data Factory y Databricks.

## ğŸ“‹ Tabla de Contenidos

- [ğŸ—ï¸ Arquitectura](#-arquitectura)
- [âœ¨ CaracterÃ­sticas](#-caracterÃ­sticas)
- [ğŸ”§ Componentes](#-componentes)
- [âš™ï¸ ConfiguraciÃ³n](#-configuraciÃ³n)
- [ğŸš€ Uso](#-uso)
- [ğŸ“Š Pipeline de Datos](#-pipeline-de-datos)
- [ğŸ“” Transformaciones de Databricks](#-transformaciones-de-databricks)
- [ğŸ”’ Seguridad](#-seguridad)
- [ğŸ“ Estructura del Proyecto](#-estructura-del-proyecto)
- [ğŸ¤ ContribuciÃ³n](#-contribuciÃ³n)

## ğŸ—ï¸ Arquitectura

Este proyecto implementa una arquitectura moderna de ingesta de datos con los siguientes componentes:

```mermaid
graph TB
    A[Kaggle Dataset] --> B[GitHub Actions]
    B --> C[Azure Key Vault]
    B --> D[Azure Data Lake Storage - Raw]
    E[Azure Data Factory] --> F[ADLS - Bronze]
    D --> F
    F --> G[ADLS - Silver]
    G --> H[ADLS - Gold]
    I[Databricks] --> F
    I --> G
    I --> H
    E --> I
```

## âœ¨ CaracterÃ­sticas

- **ğŸ”„ Ingesta Automatizada**: ExtracciÃ³n de datasets desde Kaggle mediante GitHub Actions
- **â˜ï¸ Almacenamiento en la Nube**: IntegraciÃ³n completa con Azure Data Lake Storage
- **ğŸ—ï¸ Arquitectura Medallion**: Procesamiento de datos en capas Bronze, Silver y Gold
- **ğŸ”’ GestiÃ³n Segura de Credenciales**: Uso de Azure Key Vault para secrets
- **ğŸ“Š OrquestaciÃ³n**: Azure Data Factory para coordinaciÃ³n de pipelines
- **âš¡ Procesamiento Escalable**: Databricks para transformaciones complejas
- **ğŸ¯ Formatos Optimizados**: ConversiÃ³n automÃ¡tica de Excel/CSV a Parquet

## ğŸ”§ Componentes

### 1. Kaggle Ingester (`kaggle-ingester/`)
Script principal en Python que:
- Descarga datasets desde Kaggle API
- Convierte archivos Excel a CSV
- Sube archivos a Azure Data Lake Storage
- Gestiona credenciales de forma segura

### 2. GitHub Actions Workflow (`.github/workflows/`)
AutomatizaciÃ³n CI/CD que:
- Se ejecuta mediante workflow_dispatch
- Configura el entorno Python
- Ejecuta el ingester con parÃ¡metros
- Maneja autenticaciÃ³n con Azure

### 3. Azure Data Factory Pipeline (`Data-Ingester-Pipeline/`)
Orquestador que:
- Dispara GitHub Actions via API
- Mueve datos de Raw a Bronze (CSV â†’ Parquet)
- Ejecuta notebooks de Databricks
- Limpia archivos temporales

### 4. Notebooks de Databricks (`transformation/`)
Notebooks de PySpark y SQL para procesamiento de datos:
- **Bronze**: Ingesta inicial con timestamps
- **Silver**: Limpieza, tipado y enriquecimiento
- **Gold**: Agregaciones y mÃ©tricas de negocio
- **Seguridad**: DDLs y gestiÃ³n de permisos
- **Rollback**: Scripts de limpieza y recuperaciÃ³n

## âš™ï¸ ConfiguraciÃ³n

### Prerrequisitos

- **Azure Subscription** con los siguientes servicios:
  - Azure Data Factory
  - Azure Data Lake Storage Gen2
  - Azure Key Vault
  - Azure Databricks
- **GitHub Repository** con Actions habilitadas
- **Kaggle Account** con API credentials

### Variables de Entorno y Secrets

#### Azure Key Vault Secrets:
```bash
kaggle-username    # Tu username de Kaggle
kaggle-key        # Tu API key de Kaggle
github-token      # Personal Access Token de GitHub
```

#### GitHub Repository Secrets:
```bash
AZURE_CREDENTIALS  # Service Principal JSON
KEYVAULT_URI      # https://tu-keyvault.vault.azure.net/
STORAGE_ACCOUNT   # Nombre de tu storage account
```

### InstalaciÃ³n Local

1. **Clonar el repositorio:**
   ```bash
   git clone https://github.com/gquintal/data-ingester-adb.git
   cd data-ingester-adb
   ```

2. **Crear entorno virtual:**
   ```bash
   python -m venv venv
   source venv/bin/activate  # En Windows: venv\Scripts\activate
   ```

3. **Instalar dependencias:**
   ```bash
   pip install -r kaggle-ingester/requirements.txt
   ```

## ğŸš€ Uso

### EjecuciÃ³n Manual via GitHub Actions

1. Ve a la pestaÃ±a **Actions** en tu repositorio
2. Selecciona **ğŸ”„ Kaggle to ADLS Ingester**
3. Haz clic en **Run workflow**
4. Especifica los parÃ¡metros:
   - **Dataset**: `username/dataset-name` (ej: `jillanisofttech/flight-price-prediction-dataset`)
   - **Container**: `raw` (o el nombre de tu contenedor)

### EjecuciÃ³n via Azure Data Factory

1. Abre tu instancia de Azure Data Factory
2. Navega al pipeline **Data Ingester Smart Data**
3. Ejecuta con parÃ¡metros:
   ```json
   {
     "dataset": "jillanisofttech/flight-price-prediction-dataset",
     "container_name": "raw"
   }
   ```

### EjecuciÃ³n Local (Desarrollo)

```bash
python kaggle-ingester/kaggle-ingester.py \
  --dataset "username/dataset-name" \
  --keyvault-uri "https://tu-keyvault.vault.azure.net/" \
  --storage-account "tu-storage-account" \
  --container "raw"
```

## ğŸ“Š Pipeline de Datos

El pipeline sigue una arquitectura medallion con las siguientes etapas:

### ğŸ¥‰ Raw Layer
- **Formato**: CSV/Excel originales
- **PropÃ³sito**: Datos crudos sin procesamiento
- **UbicaciÃ³n**: `container/raw/`

### ğŸ¥‰ Bronze Layer
- **PropÃ³sito**: Datos estructurados, sin transformaciones
- **Procesamiento**: ConversiÃ³n de formato, validaciÃ³n bÃ¡sica
- **UbicaciÃ³n**: `container/bronze/`

### ğŸ¥ˆ Silver Layer
- **PropÃ³sito**: Datos limpios y estandarizados
- **Procesamiento**: Limpieza, deduplicaciÃ³n, tipado
- **UbicaciÃ³n**: `container/silver/`

### ğŸ¥‡ Gold Layer
- **PropÃ³sito**: Datos listos para anÃ¡lisis
- **Procesamiento**: Agregaciones, mÃ©tricas de negocio
- **UbicaciÃ³n**: `container/gold/`

## ğŸ“” Transformaciones de Databricks

### ğŸ¥‰ Bronze Layer (`extraction to bronze.py`)
**Objetivo**: Ingesta inicial de datos con metadatos
```python
# Principales operaciones:
- Lectura de archivos Parquet desde ADLS
- Agregado de timestamp de ingesta
- Escritura en formato Delta Lake
```

### ğŸ¥ˆ Silver Layer (`transform to silver.py`)
**Objetivo**: Limpieza y estandarizaciÃ³n de datos
```python
# Transformaciones aplicadas:
- EliminaciÃ³n de duplicados
- ConversiÃ³n de tipos de datos (fechas, decimales)
- Parsing de duraciÃ³n de vuelos a minutos
- EstandarizaciÃ³n de Total_Stops (texto â†’ nÃºmeros)
- IdentificaciÃ³n de vuelos directos
- ValidaciÃ³n y limpieza de campos
```

**Campos procesados**:
- `Date_of_Journey`: ConversiÃ³n a formato DATE
- `Duration`: Parsing a `Duration_Minutes` (entero)
- `Total_Stops`: NormalizaciÃ³n de texto a nÃºmeros
- `Price`: ConversiÃ³n a DECIMAL(10,2)
- `Is_Direct_Flight`: Flag booleano para vuelos sin escalas

### ğŸ¥‡ Gold Layer (`load to gold.py`)
**Objetivo**: CreaciÃ³n de mÃ©tricas de negocio y dimensiones analÃ­ticas

#### Tabla `dim_airlines`:
```sql
-- MÃ©tricas por aerolÃ­nea:
- total_flights: Cantidad total de vuelos
- avg_price: Precio promedio
- most_common_route: Ruta mÃ¡s frecuente
- direct_flights_percentage: % de vuelos directos
- avg_stops: Promedio de escalas
```

#### Tabla `dim_routes`:
```sql
-- MÃ©tricas por ruta:
- total_flights: Vuelos en la ruta
- avg_price, min_price, max_price: EstadÃ­sticas de precios
- median_price: Mediana de precios
- avg_stops: Escalas promedio
- airlines_count: AerolÃ­neas que operan la ruta
```

### ğŸ”§ GestiÃ³n de Esquemas (`DDLs.sql`)
**ConfiguraciÃ³n inicial de la base de datos**:
- CreaciÃ³n del catÃ¡logo
- DefiniciÃ³n de esquemas: `bronze`, `silver`, `gold`
- ConfiguraciÃ³n de tablas Delta con ubicaciones ADLS
- GestiÃ³n de permisos para usuarios

### ğŸ§¹ Rollback (`Drop.sql`)
**Scripts de limpieza y recuperaciÃ³n**:
- EliminaciÃ³n de tablas Delta
- Limpieza de archivos en ADLS
- RecuperaciÃ³n de external locations
- Procedimientos de rollback para desarrollo/testing

## ğŸ”’ Seguridad

- **ğŸ” Azure Key Vault**: Almacenamiento seguro de credenciales
- **ğŸ›¡ï¸ Managed Identity**: AutenticaciÃ³n sin credenciales hardcodeadas
- **ğŸ”’ GitHub Secrets**: Variables sensibles encriptadas
- **ğŸš« .gitignore**: ExclusiÃ³n de archivos temporales y credenciales
- **âœ… Principio de menor privilegio**: Permisos mÃ­nimos necesarios

## ğŸ“ Estructura del Proyecto

```
data-ingester-adb/
â”œâ”€â”€ ğŸ“‚ .github/workflows/
â”‚   â””â”€â”€ kaggle-ingest.yml          # GitHub Actions workflow
â”œâ”€â”€ ğŸ“‚ kaggle-ingester/
â”‚   â”œâ”€â”€ kaggle-ingester.py         # Script principal de ingesta
â”‚   â””â”€â”€ requirements.txt           # Dependencias Python
â”œâ”€â”€ ğŸ“‚ Data-Ingester-Pipeline/
â”‚   â”œâ”€â”€ ğŸ“‚ dataset/
â”‚   â”‚   â”œâ”€â”€ DS_Source.json         # Dataset source definition
â”‚   â”‚   â””â”€â”€ DS_Sink.json          # Dataset sink definition
â”‚   â”œâ”€â”€ ğŸ“‚ linkedService/
â”‚   â”‚   â”œâ”€â”€ LS_ADLS.json          # Azure Data Lake linked service
â”‚   â”‚   â”œâ”€â”€ LS_AKV.json           # Azure Key Vault linked service
â”‚   â”‚   â””â”€â”€ LS_Databricks.json    # Databricks linked service
â”‚   â””â”€â”€ ğŸ“‚ pipeline/
â”‚       â””â”€â”€ Data Ingester Smart Data.json  # Main ADF pipeline
â”œâ”€â”€ ğŸ“‚ transformation/             # Databricks Notebooks
â”‚   â”œâ”€â”€ ğŸ“‚ proceso/
â”‚   â”‚   â”œâ”€â”€ extraction to bronze.py    # Bronze layer processing
â”‚   â”‚   â”œâ”€â”€ transform to silver.py     # Silver layer transformations
â”‚   â”‚   â””â”€â”€ load to gold.py           # Gold layer aggregations
â”‚   â”œâ”€â”€ ğŸ“‚ rollback/
â”‚   â”‚   â””â”€â”€ Drop.sql                  # Cleanup and rollback scripts
â”‚   â””â”€â”€ ğŸ“‚ seguridad/
â”‚       â””â”€â”€ DDLs.sql                  # Database schemas and permissions
â”œâ”€â”€ .gitignore                     # Git ignore patterns
â”œâ”€â”€ README.md                      # Este archivo
â””â”€â”€ Flights Report.lvdash.json     # Dashboard configuration
```

## ğŸ”„ Flujo de Trabajo

1. **Trigger**: Azure Data Factory inicia el pipeline
2. **Authentication**: Obtiene token de GitHub desde Key Vault
3. **GitHub Action**: Dispara workflow via API
4. **Download**: GitHub Actions descarga dataset de Kaggle
5. **Upload**: Sube archivos CSV a ADLS Raw
6. **Transform**: ADF convierte CSV a Parquet (Bronze)
7. **Process**: Databricks notebooks procesan Silver y Gold
8. **Cleanup**: Elimina archivos temporales

## ğŸ› ï¸ Troubleshooting

### Errores Comunes

**Error de autenticaciÃ³n con Kaggle:**
```bash
# Verificar que las credenciales estÃ¡n en Key Vault
# Comprobar que el usuario tiene acceso al dataset
```

**Error de conexiÃ³n con Azure:**
```bash
# Verificar que el Service Principal tiene permisos
# Comprobar que el Storage Account existe
```

**Error en GitHub Actions:**
```bash
# Verificar que los secrets estÃ¡n configurados
# Comprobar que el workflow tiene permisos
```

## ğŸ¤ ContribuciÃ³n

1. Fork el proyecto
2. Crea una rama para tu feature (`git checkout -b feature/AmazingFeature`)
3. Commit tus cambios (`git commit -m 'Add some AmazingFeature'`)
4. Push a la rama (`git push origin feature/AmazingFeature`)
5. Abre un Pull Request

## ğŸ“ Licencia

Este proyecto estÃ¡ bajo la licencia MIT. Ver `LICENSE` para mÃ¡s detalles.

## ğŸ‘¨â€ğŸ’» Autor

**Guadalupe Quintal V**
- GitHub: [@gquintal](https://github.com/gquintal)
- LinkedIn: [Guadalupe Quintal](https://linkedin.com/in/guadalupe-quintal)

---

â­ Â¡Si este proyecto te fue Ãºtil, considera darle una estrella!

ğŸ“§ Para preguntas o sugerencias, no dudes en abrir un issue.
